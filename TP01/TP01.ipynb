{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 1 : Présentation des Bibliothèques Clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset() # Obtenir l'etat initial de l'environnement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 2: Exercices Pratiques avec OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(2)\n",
      "Espace d'observations : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action: 0, Observation: [ 0.02722239 -0.2275757   0.00948014  0.28283426], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02267088 -0.4228316   0.01513682  0.57849205], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01421425 -0.227925    0.02670666  0.2906157 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00965575 -0.03319385  0.03251898  0.00647397], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00899187 -0.22876671  0.03264846  0.30923706], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00441654 -0.03412478  0.0388332   0.0270267 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00373404 -0.2297815   0.03937373  0.3317046 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00086159 -0.03524147  0.04600782  0.0516936 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00156642  0.15919162  0.0470417  -0.22612588], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00161741  0.3536108   0.04251917 -0.5036069 ], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")\n",
    "\n",
    "for _ in range(10):\n",
    "    actions = env.action_space.sample() # Action aléatoire\n",
    "    observation, reward, done, info, _ = env.step(actions) # Execute les actions et récupere les resultats \n",
    "    \n",
    "    print(f\"Action: {actions}, Observation: {observation}, Reward: {reward}\")\n",
    "    \n",
    "    if done:\n",
    "        observation, _ = env.reset()  \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Observation: [-0.03762667 -0.20267443  0.01559095  0.27659065], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04168016 -0.39801532  0.02112276  0.5741499 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04964047 -0.59342694  0.03260576  0.87341154], Reward: 1.0\n",
      "Action: 1, Observation: [-0.061509   -0.39876315  0.05007399  0.59115547], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06948426 -0.59454906  0.0618971   0.89918184], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08137525 -0.79045284  0.07988074  1.2106608 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0971843  -0.98651004  0.10409395  1.5272689 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.1169145  -1.1827227   0.13463934  1.850543  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.14056896 -1.379045    0.1716502   2.1818259 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.16814986 -1.1859555   0.21528672  1.9466659 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.19186896 -1.382624    0.25422004  2.2976649 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.21952145 -1.5790325   0.30017334  2.656747  ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.2511021 -1.3871529  0.3533083  2.468729 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.27884516 -1.5831562   0.40268287  2.8462994 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.31050828 -1.7781602   0.45960885  3.2306237 ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.34607148 -1.5869758   0.5242213   3.1040246 ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.377811  -1.3968487  0.5863018  3.0042896], Reward: 0.0\n",
      "Action: 1, Observation: [-0.40574798 -1.2077012   0.6463876   2.9306176 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.429902  -1.3995786  0.7049999  3.337449 ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.45789358 -1.2103086   0.77174896  3.3117456 ], Reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LOQ\\Documents\\ENIAD\\s4\\ML2\\TPs\\reinforcement-learning\\venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:214: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset()\n",
    "for i in range(20):\n",
    "    actions = env.action_space.sample() # choisit une action aleatoire.\n",
    "    observation,reward , done , info , _ = env.step(actions) # retourne ,l'action l'observation , la récompense\n",
    "    print(f\"Action: {actions}, Observation: {observation}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.03842044 -0.18144798  0.03492817  0.2750937 ], Récompense: 1.0\n",
      "Observation: [ 0.03479148  0.01315866  0.04043005 -0.00637143], Récompense: 1.0\n",
      "Observation: [ 0.03505465  0.20767817  0.04030262 -0.28602907], Récompense: 1.0\n",
      "Observation: [0.03920821 0.0120053  0.03458204 0.01908766], Récompense: 1.0\n",
      "Observation: [ 0.03944832 -0.18359509  0.03496379  0.3224779 ], Récompense: 1.0\n",
      "Observation: [ 0.03577642 -0.37919703  0.04141335  0.62597877], Récompense: 1.0\n",
      "Observation: [ 0.02819248 -0.57487184  0.05393292  0.9314114 ], Récompense: 1.0\n",
      "Observation: [ 0.01669504 -0.7706785   0.07256115  1.2405429 ], Récompense: 1.0\n",
      "Observation: [ 1.2814682e-03 -9.6665323e-01  9.7372010e-02  1.5550457e+00], Récompense: 1.0\n",
      "Observation: [-0.0180516  -1.1627978   0.12847292  1.8764511 ], Récompense: 1.0\n",
      "Observation: [-0.04130755 -1.3590662   0.16600195  2.2060947 ], Récompense: 1.0\n",
      "Observation: [-0.06848888 -1.5553486   0.21012384  2.5450516 ], Récompense: 1.0\n",
      "Durée totale de l'épisode : 12 étapes\n"
     ]
    }
   ],
   "source": [
    "observation, _ = env.reset() # renitialise et recupere etat init de env\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    action = int(input(\"Entrez une action (0 ou 1) : \"))  \n",
    "    if action not in [0, 1]:\n",
    "        print(\"Action invalide, veuillez entrer 0 ou 1.\")\n",
    "        continue\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # execute action et recupere\n",
    "    print(f\"Observation: {observation}, Récompense: {reward}\") #observation et reward apres action\n",
    "\n",
    "    steps += 1\n",
    "    done = terminated or truncated  # truncated limite temps\n",
    "\n",
    "print(f\"Durée totale de l'épisode : {steps} étapes\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée moyenne d'un épisode après 10 épisodes : 16.4 étapes\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "num_episodes = 10  # Nombre d'épisodes à exécuter\n",
    "total_steps = 0\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation, _ = env.reset()  # Reinitialise l'environnement et récupere l'observation initiale\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    # Exécuter un épisode avec des actions aléatoires\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aleatoire\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)  # Execute l'action\n",
    "        steps += 1\n",
    "        done = terminated or truncated  # Verifie la fin de l'épisode\n",
    "\n",
    "    total_steps += steps  # Additionner le nombre d'étapes de chaque épisode\n",
    "\n",
    "# Calcul de la durée moyenne\n",
    "average_steps = total_steps / num_episodes\n",
    "print(f\"Durée moyenne d'un épisode après {num_episodes} épisodes : {average_steps} étapes\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">EL HACHYMI Ahmed Yassine</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
